# StatQA

## Overview

Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical scenarios remains underexplored. To bridge this gap, we introduce StatQA, a synthesized benchmark comprising 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities particularly for hypothesis testing methods. Our systematic experiments, employing various prompting strategies on representative LLMs, indicate that even the most advanced models like GPT-4o achieve a maximum performance of only 64.83%, underscoring significant potential for further enhancement. Notably, while open-source LLMs show limited capability, fine-tuned models exhibit marked improvements, outperforming all prompt-based methods. Additionally, we conduct comparative human experiments with six participants, identifying a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make errors because of task confusion. This divergence in error patterns not only underscores distinct areas of proficiency and deficiency but also suggests areas where LLMs and human expertise could complement each other, inviting more in-depth investigation into their collaborative potential.

## Environment Setup

Install required python libraries from requirements.txt:

```bash
pip install -r requirements.txt
```

OpenAI API key will be used in this project, so please set your own API key in gpt_config.txt like this:

```txt
https://api.openai.com/v1/chat/completions
yourapikey
```

## Benchmark

For our benchmark StatQA and mini-StatQA, we provide both CSV and JSON formatted benchmark versions in `StatQA/` file. If you want to conduct evaluations using them, please go to the **Evaluation** section.

If you want to try the process of StatQA and mini-StatQA construction on your own, we provide the script for you:

Preprocessing and information extraction:

```bash
Script\info_extraction.sh
```

Benchmark construction:

```bash
sh Script\benchmark_construction.sh
```

Obtained StatQA and mini-StatQA benchmark will be stored in `Data\Integrated Dataset\Balanced Benchmark`. Note that this process can take hours and consume your API tokens, so please be patient or directly use the benchmark we already offered.

## Evaluation

We evaluate LLMs' capabilities based on mini-StatQA. The responses or answers generated by LLMs will be stored in `Model Answer\Origin Answer`.

The first step is to generate certain prompts for different prompting strategies:

```bash
sh Script\prompt_organization.sh
```

### For LLaMA-2/3:

To perform experiments on LLaMA-2 and LLaMA-3 models, please configure your own model path in `Evaluation\llama_evaluation.py`, and you can also set the `parallel_num` depending on your GPUs.

```python
# Path settings
if model_type == '2_7b':
    model_path = "/your_path_to/Llama-2-7b-chat-hf"
    parallel_num = 4
elif model_type == '2_13b':
    model_path = "/your_path_to/Llama-2-13b-chat-hf"
    parallel_num = 8
elif model_type == '3_8b_instruct':
    model_path = "/your_path_to/Meta-Llama-3-8B-Instruct"
    parallel_num = 8
elif model_type == '3_8b':
    model_path = "/your_path_to/Meta-Llama-3-8B"
    parallel_num = 8
elif model_type == 'sft':
    model_path = "/your_path_to/your_sft_model"
    parallel_num = 8
```

Then, you can perform evaluations for LLaMA-2/3 models by:

```bash
python Evaluation/llama_evaluation.py 
	--model_type "2_7b" # model type, e.g. LLaMA-2-7b
	--trick "zero-shot" # prompting strategies
```

We also provide `llama_exp.sh` script for you to run evaluations for LLaMA-2-7b/13b and LLaMA-3-8b(w/ and w/o Instruct) with different prompting strategies, you can modify it to meet your needs and run it by this:

```bash
sh Script\llama_exp.sh
```

### For GPT models:

Please ensure your OpenAI API key is correctly set and note that the evaluation of GPT models will consume your API tokens.

You can perform evaluations for GPT models by:

```bash
python Evaluation/gpt_evaluation.py 
	--selected_model "gpt-3.5-turbo" # model type, e.g. gpt-3.5-turbo
	--trick "zero-shot" # prompting strategies
```

We also provide `gpt_exp.sh` script for you to run evaluations for ChatGPT (gpt-3.5-turbo), GPT-4 and recently released GPT-4o with different prompting strategies, you can modify it to meet your needs and run it by this:

```bash
sh Script\gpt_exp.sh
```

## Fine-tuning

### Format Transformation for Training Set

We use a similar procedure but different source tabular data to obtain the training set: `Data\Integrated Dataset\Dataset with Prompt\Training Set\Training Set for zero-shot.csv`.

....



## Analysis

To analyze LLMs' answers and evaluate their capabilities, you can run `Script\answer_analysis.sh` script for preprocessing, $Acc(C,M)$ calculation, performance and error analysis, then summarize to tables.  We also provide data visualization with radar charts, stacked bar charts, and task confusion matrics, which can be found in `Chart/`.

```bash
sh Script\answer_analysis.sh
```

## License
APACHE LICENSE, VERSION 2.0.


## Citation

